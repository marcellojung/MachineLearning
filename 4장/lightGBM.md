# LightGBM
(Light Gradient Boosting Machine)은 Microsoft에서 개발한 그라디언트 부스팅 프레임워크로, 성능과 효율성 면에서 매우 우수한 특성을 가지고 있습니다. 특히 대용량 데이터와 복잡한 모델링에 강점을 보입니다. LightGBM의 주요 특징과 하이퍼파라미터를 설명하겠습니다.

## LightGBM의 주요 특징

1. **속도와 메모리 효율성**:
   - LightGBM은 대규모 데이터셋에서 매우 빠르게 동작하며, 메모리 사용량도 적습니다. 이는 "Leaf-wise" 트리 성장 전략과 "Histogram-based" 방법을 사용하기 때문입니다.

2. **Leaf-wise 트리 성장**:
   - LightGBM은 기존의 level-wise 방식과 달리, "Leaf-wise" 방식으로 트리를 확장합니다. 이는 트리의 가장 깊은 노드를 먼저 분할하여 더 복잡한 패턴을 학습할 수 있게 합니다. 단, 과적합(overfitting) 위험이 있을 수 있습니다.

3. **Histogram 기반의 학습**:
   - 연속형 변수를 히스토그램으로 변환하여, 빠르게 최적의 분할 지점을 찾을 수 있습니다. 이는 계산 효율성을 크게 높여줍니다.

4. **지원하는 다양한 손실 함수**:
   - 회귀, 분류, 순위 등의 다양한 문제 유형을 지원하는 여러 손실 함수를 제공합니다.

5. **병렬 학습**:
   - 데이터의 분산 처리 및 병렬 처리가 가능하여, 학습 속도가 매우 빠릅니다.

6. **하드웨어 최적화**:
   - GPU와 병렬 연산을 통해 학습 시간을 크게 단축할 수 있습니다.

## LightGBM의 주요 하이퍼파라미터

1. **num_leaves**:
   - 트리의 최대 리프 노드 수를 결정합니다. 트리가 복잡해질수록 학습 성능이 좋아지지만, 과적합 위험이 커집니다.

2. **max_depth**:
   - 트리의 최대 깊이를 제한합니다. `num_leaves`와 함께 과적합을 방지하기 위해 사용될 수 있습니다.

3. **learning_rate**:
   - 학습률로, 각 트리가 모델에 기여하는 비율을 조절합니다. 낮을수록 학습 속도는 느려지지만, 모델의 일반화 성능이 좋아질 수 있습니다.

4. **n_estimators**:
   - 부스팅 과정에서 생성할 트리의 수를 결정합니다. 많은 트리를 사용할수록 성능이 향상될 수 있지만, 학습 시간이 늘어납니다.

5. **min_data_in_leaf**:
   - 리프 노드가 가질 수 있는 최소 데이터 수를 설정합니다. 이 값을 높이면 과적합을 줄일 수 있습니다.

6. **feature_fraction**:
   - 트리를 만들 때 사용할 피처의 비율을 설정합니다. 과적합을 방지하기 위해 1보다 작은 값을 설정하는 것이 일반적입니다.

7. **bagging_fraction**:
   - 데이터를 샘플링할 때 사용할 비율입니다. 과적합을 줄이기 위해 사용됩니다.

8. **bagging_freq**:
   - 몇 번의 부스팅 반복마다 `bagging_fraction`을 적용할지 결정합니다.

9. **lambda_l1, lambda_l2**:
   - L1, L2 정규화를 위한 파라미터로, 모델 복잡도를 제어하는 데 사용됩니다.

10. **min_gain_to_split**:
    - 노드를 분할하기 위해 필요한 최소 손실 감소량입니다. 이 값을 높이면 모델의 복잡도를 줄일 수 있습니다.

11. **boosting_type**:
    - 부스팅의 유형을 지정합니다. 'gbdt' (전통적인 그래디언트 부스팅), 'dart' (드롭 아웃을 사용하는 부스팅), 'goss' (Gradient-based One-Side Sampling) 등이 있습니다.

이 외에도 LightGBM은 다양한 하이퍼파라미터를 제공하여 모델의 성능을 조정할 수 있으며, 일반적으로 하이퍼파라미터 튜닝을 통해 최적의 성능을 얻을 수 있습니다.