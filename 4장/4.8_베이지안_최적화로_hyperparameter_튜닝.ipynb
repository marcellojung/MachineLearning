{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8904a67d",
   "metadata": {},
   "source": [
    "### 베이지안 최적화 개요와 HyperOpt 사용법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6596709a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2.7\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import hyperopt\n",
    "\n",
    "print(hyperopt.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f893afc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting hyperopt==0.2.7\n",
      "  Downloading hyperopt-0.2.7-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.10/site-packages (from hyperopt==0.2.7) (1.26.4)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.10/site-packages (from hyperopt==0.2.7) (1.11.4)\n",
      "Requirement already satisfied: six in /opt/anaconda3/lib/python3.10/site-packages (from hyperopt==0.2.7) (1.16.0)\n",
      "Requirement already satisfied: networkx>=2.2 in /opt/anaconda3/lib/python3.10/site-packages (from hyperopt==0.2.7) (3.1)\n",
      "Requirement already satisfied: future in /opt/anaconda3/lib/python3.10/site-packages (from hyperopt==0.2.7) (0.18.3)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.10/site-packages (from hyperopt==0.2.7) (4.65.0)\n",
      "Requirement already satisfied: cloudpickle in /opt/anaconda3/lib/python3.10/site-packages (from hyperopt==0.2.7) (2.2.1)\n",
      "Collecting py4j (from hyperopt==0.2.7)\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Downloading hyperopt-0.2.7-py2.py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.5/200.5 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: py4j, hyperopt\n",
      "Successfully installed hyperopt-0.2.7 py4j-0.10.9.7\n"
     ]
    }
   ],
   "source": [
    "!pip install hyperopt==0.2.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e4fdabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp\n",
    "\n",
    "# -10 ~ 10까지 1간격을 가지는 입력 변수 x 집합값과 -15 ~ 15까지 1간격을 가지는 입력 변수  y 집합값 설정.\n",
    "search_space = {'x': hp.quniform('x', -10, 10, 1),  'y': hp.quniform('y', -15, 15, 1) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0d36412",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': <hyperopt.pyll.base.Apply at 0x132e22fe0>,\n",
       " 'y': <hyperopt.pyll.base.Apply at 0x132e22350>}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4dcf5c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import STATUS_OK\n",
    "\n",
    "# 목적 함수를 생성. 입력 변수값과 입력 변수 검색 범위를 가지는 딕셔너리를 인자로 받고, 특정 값을 반환\n",
    "def objective_func(search_space):\n",
    "    x = search_space['x']\n",
    "    y = search_space['y']\n",
    "    retval = x**2 - 20*y  # x2-20y 예제로 하는거지 실제능 \n",
    "    \n",
    "    return retval # return {'loss': retval, 'status':STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8042337e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 1206.51trial/s, best loss: -224.0]\n",
      "best: {'x': -4.0, 'y': 12.0}\n"
     ]
    }
   ],
   "source": [
    "from hyperopt import fmin, tpe, Trials   #tpe : tree of parzen estimator 알고리즘 \n",
    "import numpy as np\n",
    "\n",
    "# 입력 결괏값을 저장한 Trials 객체값 생성.\n",
    "trial_val = Trials()\n",
    "\n",
    "# 목적 함수의 최솟값을 반환하는 최적 입력 변숫값을 5번의 입력값 시도(max_evals=5)로 찾아냄.\n",
    "# trials : 입력값과 결괏값을 저장하는 Trials 객체값을 지정.\n",
    "best_01 = fmin(fn=objective_func, space=search_space, algo=tpe.suggest, max_evals=5\n",
    "               , trials=trial_val, rstate=np.random.default_rng(seed=0)\n",
    "              )\n",
    "print('best:', best_01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e6b812a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'loss': -64.0, 'status': 'ok'},\n",
       " {'loss': -184.0, 'status': 'ok'},\n",
       " {'loss': 56.0, 'status': 'ok'},\n",
       " {'loss': -224.0, 'status': 'ok'},\n",
       " {'loss': 61.0, 'status': 'ok'},\n",
       " {'loss': -296.0, 'status': 'ok'},\n",
       " {'loss': -40.0, 'status': 'ok'},\n",
       " {'loss': 281.0, 'status': 'ok'},\n",
       " {'loss': 64.0, 'status': 'ok'},\n",
       " {'loss': 100.0, 'status': 'ok'},\n",
       " {'loss': 60.0, 'status': 'ok'},\n",
       " {'loss': -39.0, 'status': 'ok'},\n",
       " {'loss': 1.0, 'status': 'ok'},\n",
       " {'loss': -164.0, 'status': 'ok'},\n",
       " {'loss': 21.0, 'status': 'ok'},\n",
       " {'loss': -56.0, 'status': 'ok'},\n",
       " {'loss': 284.0, 'status': 'ok'},\n",
       " {'loss': 176.0, 'status': 'ok'},\n",
       " {'loss': -171.0, 'status': 'ok'},\n",
       " {'loss': 0.0, 'status': 'ok'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_val.results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2bbbd861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 1208.02trial/s, best loss: -296.0]\n",
      "best: {'x': 2.0, 'y': 15.0}\n"
     ]
    }
   ],
   "source": [
    "trial_val = Trials()\n",
    "\n",
    "# max_evals를 20회로 늘려서 재테스트\n",
    "best_02 = fmin(fn=objective_func, space=search_space, algo=tpe.suggest, max_evals=20\n",
    "               , trials=trial_val, rstate=np.random.default_rng(seed=0))\n",
    "print('best:', best_02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "22cd2e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-6.0 5.0 {'loss': -64.0, 'status': 'ok'}\n",
      "-4.0 10.0 {'loss': -184.0, 'status': 'ok'}\n",
      "4.0 -2.0 {'loss': 56.0, 'status': 'ok'}\n",
      "-4.0 12.0 {'loss': -224.0, 'status': 'ok'}\n",
      "9.0 1.0 {'loss': 61.0, 'status': 'ok'}\n",
      "2.0 15.0 {'loss': -296.0, 'status': 'ok'}\n",
      "10.0 7.0 {'loss': -40.0, 'status': 'ok'}\n",
      "-9.0 -10.0 {'loss': 281.0, 'status': 'ok'}\n",
      "-8.0 0.0 {'loss': 64.0, 'status': 'ok'}\n",
      "-0.0 -5.0 {'loss': 100.0, 'status': 'ok'}\n",
      "-0.0 -3.0 {'loss': 60.0, 'status': 'ok'}\n",
      "1.0 2.0 {'loss': -39.0, 'status': 'ok'}\n",
      "9.0 4.0 {'loss': 1.0, 'status': 'ok'}\n",
      "6.0 10.0 {'loss': -164.0, 'status': 'ok'}\n",
      "9.0 3.0 {'loss': 21.0, 'status': 'ok'}\n",
      "2.0 3.0 {'loss': -56.0, 'status': 'ok'}\n",
      "-2.0 -14.0 {'loss': 284.0, 'status': 'ok'}\n",
      "-4.0 -8.0 {'loss': 176.0, 'status': 'ok'}\n",
      "7.0 11.0 {'loss': -171.0, 'status': 'ok'}\n",
      "-0.0 -0.0 {'loss': 0.0, 'status': 'ok'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(len(trial_val.results)):\n",
    "    print(trial_val.vals['x'][i], trial_val.vals['y'][i], end=' ')\n",
    "    print(trial_val.results[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8e06c5",
   "metadata": {},
   "source": [
    "* HyperOpt 수행 시 적용된 입력 값들과 목적 함수 반환값 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6624c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'loss': -64.0, 'status': 'ok'}, {'loss': -184.0, 'status': 'ok'}, {'loss': 56.0, 'status': 'ok'}, {'loss': -224.0, 'status': 'ok'}, {'loss': 61.0, 'status': 'ok'}, {'loss': -296.0, 'status': 'ok'}, {'loss': -40.0, 'status': 'ok'}, {'loss': 281.0, 'status': 'ok'}, {'loss': 64.0, 'status': 'ok'}, {'loss': 100.0, 'status': 'ok'}, {'loss': 60.0, 'status': 'ok'}, {'loss': -39.0, 'status': 'ok'}, {'loss': 1.0, 'status': 'ok'}, {'loss': -164.0, 'status': 'ok'}, {'loss': 21.0, 'status': 'ok'}, {'loss': -56.0, 'status': 'ok'}, {'loss': 284.0, 'status': 'ok'}, {'loss': 176.0, 'status': 'ok'}, {'loss': -171.0, 'status': 'ok'}, {'loss': 0.0, 'status': 'ok'}]\n"
     ]
    }
   ],
   "source": [
    "# fmin( )에 인자로 들어가는 Trials 객체의 result 속성에 파이썬 리스트로 목적 함수 반환값들이 저장됨\n",
    "# 리스트 내부의 개별 원소는 {'loss':함수 반환값, 'status':반환 상태값} 와 같은 딕셔너리임. \n",
    "print(trial_val.results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7cafa7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': [-6.0, -4.0, 4.0, -4.0, 9.0, 2.0, 10.0, -9.0, -8.0, -0.0, -0.0, 1.0, 9.0, 6.0, 9.0, 2.0, -2.0, -4.0, 7.0, -0.0], 'y': [5.0, 10.0, -2.0, 12.0, 1.0, 15.0, 7.0, -10.0, 0.0, -5.0, -3.0, 2.0, 4.0, 10.0, 3.0, 3.0, -14.0, -8.0, 11.0, -0.0]}\n"
     ]
    }
   ],
   "source": [
    "# Trials 객체의 vals 속성에 {'입력변수명':개별 수행 시마다 입력된 값 리스트} 형태로 저장됨.\n",
    "print(trial_val.vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6272c51f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>losses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-184.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>56.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-4.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-224.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>61.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>-296.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-9.0</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>281.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-164.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-56.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-2.0</td>\n",
       "      <td>-14.0</td>\n",
       "      <td>284.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-4.0</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>176.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>7.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>-171.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       x     y  losses\n",
       "0   -6.0   5.0   -64.0\n",
       "1   -4.0  10.0  -184.0\n",
       "2    4.0  -2.0    56.0\n",
       "3   -4.0  12.0  -224.0\n",
       "4    9.0   1.0    61.0\n",
       "5    2.0  15.0  -296.0\n",
       "6   10.0   7.0   -40.0\n",
       "7   -9.0 -10.0   281.0\n",
       "8   -8.0   0.0    64.0\n",
       "9   -0.0  -5.0   100.0\n",
       "10  -0.0  -3.0    60.0\n",
       "11   1.0   2.0   -39.0\n",
       "12   9.0   4.0     1.0\n",
       "13   6.0  10.0  -164.0\n",
       "14   9.0   3.0    21.0\n",
       "15   2.0   3.0   -56.0\n",
       "16  -2.0 -14.0   284.0\n",
       "17  -4.0  -8.0   176.0\n",
       "18   7.0  11.0  -171.0\n",
       "19  -0.0  -0.0     0.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "# results에서 loss 키값에 해당하는 밸류들을 추출하여 list로 생성. \n",
    "losses = [loss_dict['loss'] for loss_dict in trial_val.results]\n",
    "\n",
    "# DataFrame으로 생성. \n",
    "result_df = pd.DataFrame({'x': trial_val.vals['x'],\n",
    "                         'y': trial_val.vals['y'],\n",
    "                          'losses': losses\n",
    "                         }\n",
    "                        )\n",
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c74b02",
   "metadata": {},
   "source": [
    "### HyperOpt를 XGBoost 하이퍼 파라미터 튜닝에 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "577be02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "dataset = load_breast_cancer()\n",
    "\n",
    "cancer_df = pd.DataFrame(data=dataset.data, columns=dataset.feature_names)\n",
    "cancer_df['target']= dataset.target\n",
    "X_features = cancer_df.iloc[:, :-1]\n",
    "y_label = cancer_df.iloc[:, -1]\n",
    "\n",
    "# 전체 데이터 중 80%는 학습용 데이터, 20%는 테스트용 데이터 추출\n",
    "X_train, X_test, y_train, y_test=train_test_split(X_features, y_label,\n",
    "                                         test_size=0.2, random_state=156 )\n",
    "\n",
    "# 학습 데이터를 다시 학습과 검증 데이터로 분리 \n",
    "X_tr, X_val, y_tr, y_val= train_test_split(X_train, y_train,\n",
    "                                         test_size=0.1, random_state=156 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f0578d",
   "metadata": {},
   "source": [
    "![](./hyperopt.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1cc93692",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp\n",
    "\n",
    "# max_depth는 5에서 20까지 1간격으로, min_child_weight는 1에서 2까지 1간격으로\n",
    "# colsample_bytree는 0.5에서 1사이, learning_rate는 0.01에서 0.2사이 정규 분포된 값으로 검색. \n",
    "# 정수형 값을 점전적으로 증가시킬 때 quniform() 함수 사용.\n",
    "# uniform : 0부터 최대까지 정규분포로 값을 뽑아줌\n",
    "xgb_search_space = {'max_depth': hp.quniform('max_depth', 5, 20, 1),  #max_depth : 트리의 최대 깊이\n",
    "                    'min_child_weight': hp.quniform('min_child_weight', 1, 2, 1),  #min_child_weight : 트리에서 추가적으로 가지를 나눌지 결정하기 위해 필요한 데이터들의 weight 총합\n",
    "                    'learning_rate': hp.uniform('learning_rate', 0.01, 0.2),    #learning_rate : 학습률\n",
    "                    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1)  #colsample_bytree : 트리를\n",
    "                    \n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "af2b0e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from xgboost import XGBClassifier\n",
    "from hyperopt import STATUS_OK\n",
    "\n",
    "# fmin()에서 입력된 search_space값으로 입력된 모든 값은 실수형임. \n",
    "# XGBClassifier의 정수형 하이퍼 파라미터는 정수형 변환을 해줘야 함. \n",
    "# 정확도는 높은 수록 더 좋은 수치임. -1* 정확도를 곱해서 큰 정확도 값일 수록 최소가 되도록 변환\n",
    "def objective_func(search_space):\n",
    "    # 수행 시간 절약을 위해 n_estimators는 100으로 축소\n",
    "    xgb_clf = XGBClassifier(n_estimators=100, max_depth=int(search_space['max_depth']), #5.0,6.0.이렇게 들어오기 때문에 정수형으로 변환 필요\n",
    "                            min_child_weight=int(search_space['min_child_weight']),\n",
    "                            learning_rate=search_space['learning_rate'],\n",
    "                            colsample_bytree=search_space['colsample_bytree'], \n",
    "                            eval_metric='logloss')   #logloss : 로그 손실 함수\n",
    "    \n",
    "    accuracy = cross_val_score(xgb_clf, X_train, y_train, scoring='accuracy', cv=3)\n",
    "        \n",
    "    # accuracy는 cv=3 개수만큼의 정확도 결과를 가지므로 이를 평균해서 반환하되 -1을 곱해줌. \n",
    "    return {'loss':-1 * np.mean(accuracy), 'status': STATUS_OK}  #accuracy가 높을 수록 좋은 것이므로 -1을 곱해줌. fmin()은 최소값을 찾기 때문에"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "767592d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:05<00:00,  9.47trial/s, best loss: -0.9648541884512606]\n",
      "best: {'colsample_bytree': 0.6743364060621724, 'learning_rate': 0.1423918754091481, 'max_depth': 16.0, 'min_child_weight': 2.0}\n"
     ]
    }
   ],
   "source": [
    "from hyperopt import fmin, tpe, Trials\n",
    "\n",
    "trial_val = Trials()\n",
    "best = fmin(fn=objective_func,\n",
    "            space=xgb_search_space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=50, # 최대 반복 횟수를 지정합니다.\n",
    "            trials=trial_val, rstate=np.random.default_rng(seed=9))\n",
    "print('best:', best)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2099b60",
   "metadata": {},
   "source": [
    "정확도가 96일때 하이퍼파라미터들 반환 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ee280368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "colsample_bytree:0.67434, learning_rate:0.14239, max_depth:16, min_child_weight:2\n"
     ]
    }
   ],
   "source": [
    "print('colsample_bytree:{0}, learning_rate:{1}, max_depth:{2}, min_child_weight:{3}'.format(\n",
    "                        round(best['colsample_bytree'], 5), round(best['learning_rate'], 5),\n",
    "                        int(best['max_depth']), int(best['min_child_weight'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "711de322",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "\n",
    "def get_clf_eval(y_test, pred=None, pred_proba=None):\n",
    "    confusion = confusion_matrix( y_test, pred)\n",
    "    accuracy = accuracy_score(y_test , pred)\n",
    "    precision = precision_score(y_test , pred)\n",
    "    recall = recall_score(y_test , pred)\n",
    "    f1 = f1_score(y_test,pred)\n",
    "    # ROC-AUC 추가 \n",
    "    roc_auc = roc_auc_score(y_test, pred_proba)\n",
    "    print('오차 행렬')\n",
    "    print(confusion)\n",
    "    # ROC-AUC print 추가\n",
    "    print('정확도: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f},\\\n",
    "    F1: {3:.4f}, AUC:{4:.4f}'.format(accuracy, precision, recall, f1, roc_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0102c608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.1\n"
     ]
    }
   ],
   "source": [
    "import xgboost\n",
    "print(xgboost.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e6aabd2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-logloss:0.57763\n",
      "[1]\tvalidation_0-logloss:0.51353\n",
      "[2]\tvalidation_0-logloss:0.47397\n",
      "[3]\tvalidation_0-logloss:0.43285\n",
      "[4]\tvalidation_0-logloss:0.40387\n",
      "[5]\tvalidation_0-logloss:0.37609\n",
      "[6]\tvalidation_0-logloss:0.35723\n",
      "[7]\tvalidation_0-logloss:0.33985\n",
      "[8]\tvalidation_0-logloss:0.32521\n",
      "[9]\tvalidation_0-logloss:0.31862\n",
      "[10]\tvalidation_0-logloss:0.30917\n",
      "[11]\tvalidation_0-logloss:0.29975\n",
      "[12]\tvalidation_0-logloss:0.29311\n",
      "[13]\tvalidation_0-logloss:0.28698\n",
      "[14]\tvalidation_0-logloss:0.28265\n",
      "[15]\tvalidation_0-logloss:0.27860\n",
      "[16]\tvalidation_0-logloss:0.27509\n",
      "[17]\tvalidation_0-logloss:0.26845\n",
      "[18]\tvalidation_0-logloss:0.26557\n",
      "[19]\tvalidation_0-logloss:0.26429\n",
      "[20]\tvalidation_0-logloss:0.25950\n",
      "[21]\tvalidation_0-logloss:0.25823\n",
      "[22]\tvalidation_0-logloss:0.25623\n",
      "[23]\tvalidation_0-logloss:0.25646\n",
      "[24]\tvalidation_0-logloss:0.25664\n",
      "[25]\tvalidation_0-logloss:0.25535\n",
      "[26]\tvalidation_0-logloss:0.25407\n",
      "[27]\tvalidation_0-logloss:0.24970\n",
      "[28]\tvalidation_0-logloss:0.24998\n",
      "[29]\tvalidation_0-logloss:0.24721\n",
      "[30]\tvalidation_0-logloss:0.24750\n",
      "[31]\tvalidation_0-logloss:0.24876\n",
      "[32]\tvalidation_0-logloss:0.24989\n",
      "[33]\tvalidation_0-logloss:0.24825\n",
      "[34]\tvalidation_0-logloss:0.24922\n",
      "[35]\tvalidation_0-logloss:0.24586\n",
      "[36]\tvalidation_0-logloss:0.24135\n",
      "[37]\tvalidation_0-logloss:0.23880\n",
      "[38]\tvalidation_0-logloss:0.24069\n",
      "[39]\tvalidation_0-logloss:0.24142\n",
      "[40]\tvalidation_0-logloss:0.23831\n",
      "[41]\tvalidation_0-logloss:0.24167\n",
      "[42]\tvalidation_0-logloss:0.23891\n",
      "[43]\tvalidation_0-logloss:0.23933\n",
      "[44]\tvalidation_0-logloss:0.24154\n",
      "[45]\tvalidation_0-logloss:0.24161\n",
      "[46]\tvalidation_0-logloss:0.24260\n",
      "[47]\tvalidation_0-logloss:0.24098\n",
      "[48]\tvalidation_0-logloss:0.24252\n",
      "[49]\tvalidation_0-logloss:0.24470\n",
      "[50]\tvalidation_0-logloss:0.24323\n",
      "[51]\tvalidation_0-logloss:0.24275\n",
      "[52]\tvalidation_0-logloss:0.24325\n",
      "[53]\tvalidation_0-logloss:0.24183\n",
      "[54]\tvalidation_0-logloss:0.24142\n",
      "[55]\tvalidation_0-logloss:0.24177\n",
      "[56]\tvalidation_0-logloss:0.23923\n",
      "[57]\tvalidation_0-logloss:0.23784\n",
      "[58]\tvalidation_0-logloss:0.23792\n",
      "[59]\tvalidation_0-logloss:0.23973\n",
      "[60]\tvalidation_0-logloss:0.24140\n",
      "[61]\tvalidation_0-logloss:0.24014\n",
      "[62]\tvalidation_0-logloss:0.23981\n",
      "[63]\tvalidation_0-logloss:0.24014\n",
      "[64]\tvalidation_0-logloss:0.24175\n",
      "[65]\tvalidation_0-logloss:0.23936\n",
      "[66]\tvalidation_0-logloss:0.23942\n",
      "[67]\tvalidation_0-logloss:0.23823\n",
      "[68]\tvalidation_0-logloss:0.23983\n",
      "[69]\tvalidation_0-logloss:0.23957\n",
      "[70]\tvalidation_0-logloss:0.24013\n",
      "[71]\tvalidation_0-logloss:0.23895\n",
      "[72]\tvalidation_0-logloss:0.23886\n",
      "[73]\tvalidation_0-logloss:0.24041\n",
      "[74]\tvalidation_0-logloss:0.24014\n",
      "[75]\tvalidation_0-logloss:0.23905\n",
      "[76]\tvalidation_0-logloss:0.24054\n",
      "[77]\tvalidation_0-logloss:0.23835\n",
      "[78]\tvalidation_0-logloss:0.23811\n",
      "[79]\tvalidation_0-logloss:0.23700\n",
      "[80]\tvalidation_0-logloss:0.23705\n",
      "[81]\tvalidation_0-logloss:0.23755\n",
      "[82]\tvalidation_0-logloss:0.23529\n",
      "[83]\tvalidation_0-logloss:0.23678\n",
      "[84]\tvalidation_0-logloss:0.23471\n",
      "[85]\tvalidation_0-logloss:0.23609\n",
      "[86]\tvalidation_0-logloss:0.23504\n",
      "[87]\tvalidation_0-logloss:0.23509\n",
      "[88]\tvalidation_0-logloss:0.23561\n",
      "[89]\tvalidation_0-logloss:0.23459\n",
      "[90]\tvalidation_0-logloss:0.23266\n",
      "[91]\tvalidation_0-logloss:0.23402\n",
      "[92]\tvalidation_0-logloss:0.23410\n",
      "[93]\tvalidation_0-logloss:0.23313\n",
      "[94]\tvalidation_0-logloss:0.23308\n",
      "[95]\tvalidation_0-logloss:0.23282\n",
      "[96]\tvalidation_0-logloss:0.23188\n",
      "[97]\tvalidation_0-logloss:0.23268\n",
      "[98]\tvalidation_0-logloss:0.23063\n",
      "[99]\tvalidation_0-logloss:0.22885\n",
      "[100]\tvalidation_0-logloss:0.22966\n",
      "[101]\tvalidation_0-logloss:0.22774\n",
      "[102]\tvalidation_0-logloss:0.22770\n",
      "[103]\tvalidation_0-logloss:0.22683\n",
      "[104]\tvalidation_0-logloss:0.22509\n",
      "[105]\tvalidation_0-logloss:0.22484\n",
      "[106]\tvalidation_0-logloss:0.22565\n",
      "[107]\tvalidation_0-logloss:0.22704\n",
      "[108]\tvalidation_0-logloss:0.22710\n",
      "[109]\tvalidation_0-logloss:0.22708\n",
      "[110]\tvalidation_0-logloss:0.22524\n",
      "[111]\tvalidation_0-logloss:0.22597\n",
      "[112]\tvalidation_0-logloss:0.22426\n",
      "[113]\tvalidation_0-logloss:0.22436\n",
      "[114]\tvalidation_0-logloss:0.22260\n",
      "[115]\tvalidation_0-logloss:0.22237\n",
      "[116]\tvalidation_0-logloss:0.22155\n",
      "[117]\tvalidation_0-logloss:0.22228\n",
      "[118]\tvalidation_0-logloss:0.22233\n",
      "[119]\tvalidation_0-logloss:0.22370\n",
      "[120]\tvalidation_0-logloss:0.22207\n",
      "[121]\tvalidation_0-logloss:0.22221\n",
      "[122]\tvalidation_0-logloss:0.22146\n",
      "[123]\tvalidation_0-logloss:0.22149\n",
      "[124]\tvalidation_0-logloss:0.21992\n",
      "[125]\tvalidation_0-logloss:0.21997\n",
      "[126]\tvalidation_0-logloss:0.22074\n",
      "[127]\tvalidation_0-logloss:0.21915\n",
      "[128]\tvalidation_0-logloss:0.22049\n",
      "[129]\tvalidation_0-logloss:0.21899\n",
      "[130]\tvalidation_0-logloss:0.21904\n",
      "[131]\tvalidation_0-logloss:0.21975\n",
      "[132]\tvalidation_0-logloss:0.21904\n",
      "[133]\tvalidation_0-logloss:0.21876\n",
      "[134]\tvalidation_0-logloss:0.21880\n",
      "[135]\tvalidation_0-logloss:0.22009\n",
      "[136]\tvalidation_0-logloss:0.21862\n",
      "[137]\tvalidation_0-logloss:0.21935\n",
      "[138]\tvalidation_0-logloss:0.21954\n",
      "[139]\tvalidation_0-logloss:0.21810\n",
      "[140]\tvalidation_0-logloss:0.21790\n",
      "[141]\tvalidation_0-logloss:0.21808\n",
      "[142]\tvalidation_0-logloss:0.21742\n",
      "[143]\tvalidation_0-logloss:0.21715\n",
      "[144]\tvalidation_0-logloss:0.21791\n",
      "[145]\tvalidation_0-logloss:0.21734\n",
      "[146]\tvalidation_0-logloss:0.21854\n",
      "[147]\tvalidation_0-logloss:0.21857\n",
      "[148]\tvalidation_0-logloss:0.21877\n",
      "[149]\tvalidation_0-logloss:0.21739\n",
      "[150]\tvalidation_0-logloss:0.21810\n",
      "[151]\tvalidation_0-logloss:0.21791\n",
      "[152]\tvalidation_0-logloss:0.21666\n",
      "[153]\tvalidation_0-logloss:0.21686\n",
      "[154]\tvalidation_0-logloss:0.21806\n",
      "[155]\tvalidation_0-logloss:0.21744\n",
      "[156]\tvalidation_0-logloss:0.21888\n",
      "[157]\tvalidation_0-logloss:0.21860\n",
      "[158]\tvalidation_0-logloss:0.21934\n",
      "[159]\tvalidation_0-logloss:0.21806\n",
      "[160]\tvalidation_0-logloss:0.21846\n",
      "[161]\tvalidation_0-logloss:0.21787\n",
      "[162]\tvalidation_0-logloss:0.21858\n",
      "[163]\tvalidation_0-logloss:0.21799\n",
      "[164]\tvalidation_0-logloss:0.21821\n",
      "[165]\tvalidation_0-logloss:0.21793\n",
      "[166]\tvalidation_0-logloss:0.21749\n",
      "[167]\tvalidation_0-logloss:0.21863\n",
      "[168]\tvalidation_0-logloss:0.21747\n",
      "[169]\tvalidation_0-logloss:0.21770\n",
      "[170]\tvalidation_0-logloss:0.21745\n",
      "[171]\tvalidation_0-logloss:0.21815\n",
      "[172]\tvalidation_0-logloss:0.21759\n",
      "[173]\tvalidation_0-logloss:0.21894\n",
      "[174]\tvalidation_0-logloss:0.22002\n",
      "[175]\tvalidation_0-logloss:0.21941\n",
      "[176]\tvalidation_0-logloss:0.21968\n",
      "[177]\tvalidation_0-logloss:0.21913\n",
      "[178]\tvalidation_0-logloss:0.21936\n",
      "[179]\tvalidation_0-logloss:0.21910\n",
      "[180]\tvalidation_0-logloss:0.21980\n",
      "[181]\tvalidation_0-logloss:0.21927\n",
      "[182]\tvalidation_0-logloss:0.21903\n",
      "[183]\tvalidation_0-logloss:0.22008\n",
      "[184]\tvalidation_0-logloss:0.22031\n",
      "[185]\tvalidation_0-logloss:0.22009\n",
      "[186]\tvalidation_0-logloss:0.21970\n",
      "[187]\tvalidation_0-logloss:0.22036\n",
      "[188]\tvalidation_0-logloss:0.22016\n",
      "[189]\tvalidation_0-logloss:0.21965\n",
      "[190]\tvalidation_0-logloss:0.21987\n",
      "[191]\tvalidation_0-logloss:0.21963\n",
      "[192]\tvalidation_0-logloss:0.22029\n",
      "[193]\tvalidation_0-logloss:0.21979\n",
      "[194]\tvalidation_0-logloss:0.21945\n",
      "[195]\tvalidation_0-logloss:0.21921\n",
      "[196]\tvalidation_0-logloss:0.21877\n",
      "[197]\tvalidation_0-logloss:0.21855\n",
      "[198]\tvalidation_0-logloss:0.21880\n",
      "[199]\tvalidation_0-logloss:0.21946\n",
      "[200]\tvalidation_0-logloss:0.21913\n",
      "[201]\tvalidation_0-logloss:0.21937\n",
      "[202]\tvalidation_0-logloss:0.21916\n",
      "[203]\tvalidation_0-logloss:0.21869\n",
      "[204]\tvalidation_0-logloss:0.21824\n",
      "[205]\tvalidation_0-logloss:0.21847\n",
      "[206]\tvalidation_0-logloss:0.21817\n",
      "[207]\tvalidation_0-logloss:0.21878\n",
      "[208]\tvalidation_0-logloss:0.21836\n",
      "[209]\tvalidation_0-logloss:0.21815\n",
      "[210]\tvalidation_0-logloss:0.21874\n",
      "[211]\tvalidation_0-logloss:0.21853\n",
      "[212]\tvalidation_0-logloss:0.21813\n",
      "[213]\tvalidation_0-logloss:0.21836\n",
      "[214]\tvalidation_0-logloss:0.21809\n",
      "[215]\tvalidation_0-logloss:0.21789\n",
      "[216]\tvalidation_0-logloss:0.21846\n",
      "[217]\tvalidation_0-logloss:0.21824\n",
      "[218]\tvalidation_0-logloss:0.21849\n",
      "[219]\tvalidation_0-logloss:0.21823\n",
      "[220]\tvalidation_0-logloss:0.21784\n",
      "[221]\tvalidation_0-logloss:0.21839\n",
      "[222]\tvalidation_0-logloss:0.21803\n",
      "[223]\tvalidation_0-logloss:0.21827\n",
      "[224]\tvalidation_0-logloss:0.21806\n",
      "[225]\tvalidation_0-logloss:0.21783\n",
      "[226]\tvalidation_0-logloss:0.21808\n",
      "[227]\tvalidation_0-logloss:0.21788\n",
      "[228]\tvalidation_0-logloss:0.21841\n",
      "[229]\tvalidation_0-logloss:0.21864\n",
      "[230]\tvalidation_0-logloss:0.21845\n",
      "[231]\tvalidation_0-logloss:0.21895\n",
      "[232]\tvalidation_0-logloss:0.21918\n",
      "[233]\tvalidation_0-logloss:0.21896\n",
      "[234]\tvalidation_0-logloss:0.21861\n",
      "[235]\tvalidation_0-logloss:0.21841\n",
      "[236]\tvalidation_0-logloss:0.21858\n",
      "[237]\tvalidation_0-logloss:0.21880\n",
      "[238]\tvalidation_0-logloss:0.21901\n",
      "[239]\tvalidation_0-logloss:0.21882\n",
      "[240]\tvalidation_0-logloss:0.21903\n",
      "[241]\tvalidation_0-logloss:0.21868\n",
      "[242]\tvalidation_0-logloss:0.21890\n",
      "[243]\tvalidation_0-logloss:0.21858\n",
      "[244]\tvalidation_0-logloss:0.21874\n",
      "[245]\tvalidation_0-logloss:0.21856\n",
      "[246]\tvalidation_0-logloss:0.21864\n",
      "[247]\tvalidation_0-logloss:0.21849\n",
      "[248]\tvalidation_0-logloss:0.21831\n",
      "[249]\tvalidation_0-logloss:0.21839\n",
      "[250]\tvalidation_0-logloss:0.21838\n",
      "[251]\tvalidation_0-logloss:0.21880\n",
      "[252]\tvalidation_0-logloss:0.21824\n",
      "[253]\tvalidation_0-logloss:0.21840\n",
      "[254]\tvalidation_0-logloss:0.21823\n",
      "[255]\tvalidation_0-logloss:0.21830\n",
      "[256]\tvalidation_0-logloss:0.21816\n",
      "[257]\tvalidation_0-logloss:0.21786\n",
      "[258]\tvalidation_0-logloss:0.21826\n",
      "[259]\tvalidation_0-logloss:0.21772\n",
      "[260]\tvalidation_0-logloss:0.21779\n",
      "[261]\tvalidation_0-logloss:0.21727\n",
      "[262]\tvalidation_0-logloss:0.21712\n",
      "[263]\tvalidation_0-logloss:0.21720\n",
      "[264]\tvalidation_0-logloss:0.21713\n",
      "[265]\tvalidation_0-logloss:0.21700\n",
      "[266]\tvalidation_0-logloss:0.21685\n",
      "[267]\tvalidation_0-logloss:0.21671\n",
      "[268]\tvalidation_0-logloss:0.21709\n",
      "[269]\tvalidation_0-logloss:0.21717\n",
      "[270]\tvalidation_0-logloss:0.21723\n",
      "[271]\tvalidation_0-logloss:0.21728\n",
      "[272]\tvalidation_0-logloss:0.21728\n",
      "[273]\tvalidation_0-logloss:0.21722\n",
      "[274]\tvalidation_0-logloss:0.21731\n",
      "[275]\tvalidation_0-logloss:0.21739\n",
      "[276]\tvalidation_0-logloss:0.21746\n",
      "[277]\tvalidation_0-logloss:0.21752\n",
      "[278]\tvalidation_0-logloss:0.21747\n",
      "[279]\tvalidation_0-logloss:0.21732\n",
      "[280]\tvalidation_0-logloss:0.21718\n",
      "[281]\tvalidation_0-logloss:0.21724\n",
      "[282]\tvalidation_0-logloss:0.21757\n",
      "[283]\tvalidation_0-logloss:0.21763\n",
      "[284]\tvalidation_0-logloss:0.21749\n",
      "[285]\tvalidation_0-logloss:0.21744\n",
      "[286]\tvalidation_0-logloss:0.21750\n",
      "[287]\tvalidation_0-logloss:0.21745\n",
      "[288]\tvalidation_0-logloss:0.21777\n",
      "[289]\tvalidation_0-logloss:0.21773\n",
      "[290]\tvalidation_0-logloss:0.21769\n",
      "[291]\tvalidation_0-logloss:0.21769\n",
      "[292]\tvalidation_0-logloss:0.21769\n",
      "[293]\tvalidation_0-logloss:0.21769\n",
      "[294]\tvalidation_0-logloss:0.21769\n",
      "[295]\tvalidation_0-logloss:0.21769\n",
      "[296]\tvalidation_0-logloss:0.21769\n",
      "[297]\tvalidation_0-logloss:0.21769\n",
      "[298]\tvalidation_0-logloss:0.21769\n",
      "[299]\tvalidation_0-logloss:0.21769\n",
      "[300]\tvalidation_0-logloss:0.21769\n",
      "[301]\tvalidation_0-logloss:0.21769\n",
      "[302]\tvalidation_0-logloss:0.21769\n",
      "[303]\tvalidation_0-logloss:0.21769\n",
      "[304]\tvalidation_0-logloss:0.21769\n",
      "[305]\tvalidation_0-logloss:0.21769\n",
      "[306]\tvalidation_0-logloss:0.21769\n",
      "[307]\tvalidation_0-logloss:0.21769\n",
      "[308]\tvalidation_0-logloss:0.21769\n",
      "[309]\tvalidation_0-logloss:0.21769\n",
      "[310]\tvalidation_0-logloss:0.21769\n",
      "[311]\tvalidation_0-logloss:0.21769\n",
      "[312]\tvalidation_0-logloss:0.21769\n",
      "[313]\tvalidation_0-logloss:0.21769\n",
      "[314]\tvalidation_0-logloss:0.21769\n",
      "[315]\tvalidation_0-logloss:0.21769\n",
      "[316]\tvalidation_0-logloss:0.21769\n",
      "[317]\tvalidation_0-logloss:0.21769\n",
      "[318]\tvalidation_0-logloss:0.21769\n",
      "[319]\tvalidation_0-logloss:0.21769\n",
      "[320]\tvalidation_0-logloss:0.21769\n",
      "[321]\tvalidation_0-logloss:0.21769\n",
      "[322]\tvalidation_0-logloss:0.21769\n",
      "[323]\tvalidation_0-logloss:0.21769\n",
      "[324]\tvalidation_0-logloss:0.21769\n",
      "[325]\tvalidation_0-logloss:0.21769\n",
      "[326]\tvalidation_0-logloss:0.21769\n",
      "[327]\tvalidation_0-logloss:0.21769\n",
      "[328]\tvalidation_0-logloss:0.21769\n",
      "[329]\tvalidation_0-logloss:0.21769\n",
      "[330]\tvalidation_0-logloss:0.21769\n",
      "[331]\tvalidation_0-logloss:0.21769\n",
      "[332]\tvalidation_0-logloss:0.21769\n",
      "[333]\tvalidation_0-logloss:0.21769\n",
      "[334]\tvalidation_0-logloss:0.21769\n",
      "[335]\tvalidation_0-logloss:0.21769\n",
      "[336]\tvalidation_0-logloss:0.21769\n",
      "[337]\tvalidation_0-logloss:0.21769\n",
      "[338]\tvalidation_0-logloss:0.21769\n",
      "[339]\tvalidation_0-logloss:0.21769\n",
      "[340]\tvalidation_0-logloss:0.21769\n",
      "[341]\tvalidation_0-logloss:0.21769\n",
      "[342]\tvalidation_0-logloss:0.21769\n",
      "[343]\tvalidation_0-logloss:0.21769\n",
      "[344]\tvalidation_0-logloss:0.21769\n",
      "[345]\tvalidation_0-logloss:0.21769\n",
      "[346]\tvalidation_0-logloss:0.21769\n",
      "[347]\tvalidation_0-logloss:0.21769\n",
      "[348]\tvalidation_0-logloss:0.21769\n",
      "[349]\tvalidation_0-logloss:0.21769\n",
      "[350]\tvalidation_0-logloss:0.21769\n",
      "[351]\tvalidation_0-logloss:0.21769\n",
      "[352]\tvalidation_0-logloss:0.21769\n",
      "[353]\tvalidation_0-logloss:0.21769\n",
      "[354]\tvalidation_0-logloss:0.21769\n",
      "[355]\tvalidation_0-logloss:0.21769\n",
      "[356]\tvalidation_0-logloss:0.21769\n",
      "[357]\tvalidation_0-logloss:0.21769\n",
      "[358]\tvalidation_0-logloss:0.21769\n",
      "[359]\tvalidation_0-logloss:0.21769\n",
      "[360]\tvalidation_0-logloss:0.21769\n",
      "[361]\tvalidation_0-logloss:0.21769\n",
      "[362]\tvalidation_0-logloss:0.21769\n",
      "[363]\tvalidation_0-logloss:0.21769\n",
      "[364]\tvalidation_0-logloss:0.21769\n",
      "[365]\tvalidation_0-logloss:0.21769\n",
      "[366]\tvalidation_0-logloss:0.21769\n",
      "[367]\tvalidation_0-logloss:0.21769\n",
      "[368]\tvalidation_0-logloss:0.21769\n",
      "[369]\tvalidation_0-logloss:0.21769\n",
      "[370]\tvalidation_0-logloss:0.21769\n",
      "[371]\tvalidation_0-logloss:0.21769\n",
      "[372]\tvalidation_0-logloss:0.21769\n",
      "[373]\tvalidation_0-logloss:0.21769\n",
      "[374]\tvalidation_0-logloss:0.21769\n",
      "[375]\tvalidation_0-logloss:0.21769\n",
      "[376]\tvalidation_0-logloss:0.21769\n",
      "[377]\tvalidation_0-logloss:0.21769\n",
      "[378]\tvalidation_0-logloss:0.21769\n",
      "[379]\tvalidation_0-logloss:0.21769\n",
      "[380]\tvalidation_0-logloss:0.21769\n",
      "[381]\tvalidation_0-logloss:0.21769\n",
      "[382]\tvalidation_0-logloss:0.21769\n",
      "[383]\tvalidation_0-logloss:0.21769\n",
      "[384]\tvalidation_0-logloss:0.21769\n",
      "[385]\tvalidation_0-logloss:0.21769\n",
      "[386]\tvalidation_0-logloss:0.21769\n",
      "[387]\tvalidation_0-logloss:0.21769\n",
      "[388]\tvalidation_0-logloss:0.21769\n",
      "[389]\tvalidation_0-logloss:0.21769\n",
      "[390]\tvalidation_0-logloss:0.21769\n",
      "[391]\tvalidation_0-logloss:0.21769\n",
      "[392]\tvalidation_0-logloss:0.21769\n",
      "[393]\tvalidation_0-logloss:0.21769\n",
      "[394]\tvalidation_0-logloss:0.21769\n",
      "[395]\tvalidation_0-logloss:0.21769\n",
      "[396]\tvalidation_0-logloss:0.21769\n",
      "[397]\tvalidation_0-logloss:0.21769\n",
      "[398]\tvalidation_0-logloss:0.21769\n",
      "[399]\tvalidation_0-logloss:0.21769\n",
      "오차 행렬\n",
      "[[34  3]\n",
      " [ 2 75]]\n",
      "정확도: 0.9561, 정밀도: 0.9615, 재현율: 0.9740,    F1: 0.9677, AUC:0.9937\n"
     ]
    }
   ],
   "source": [
    "xgb_wrapper = XGBClassifier(n_estimators=400, learning_rate=round(best['learning_rate'], 5), \n",
    "                            max_depth=int(best['max_depth']), min_child_weight=int(best['min_child_weight']),\n",
    "                            colsample_bytree=round(best['colsample_bytree'], 5)\n",
    "                           )\n",
    "\n",
    "evals = [(X_tr, y_tr), (X_val, y_val)]\n",
    "# xgb_wrapper.fit(X_tr, y_tr, early_stopping_rounds=50, eval_metric='logloss', \n",
    "#                 eval_set=evals, verbose=True)\n",
    "    # early_stopping_rounds=50, \n",
    "    # eval_metric='logloss', \n",
    "xgb_wrapper.fit(\n",
    "    X_tr, \n",
    "    y_tr, \n",
    "\n",
    "    eval_set=[(X_val, y_val)], \n",
    "    verbose=True\n",
    ")\n",
    "preds = xgb_wrapper.predict(X_test)\n",
    "pred_proba = xgb_wrapper.predict_proba(X_test)[:, 1]\n",
    "\n",
    "get_clf_eval(y_test, preds, pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "363ef454",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_child_weight</th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>losses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.585235</td>\n",
       "      <td>0.033688</td>\n",
       "      <td>-0.947296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.727186</td>\n",
       "      <td>0.105956</td>\n",
       "      <td>-0.958275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.959945</td>\n",
       "      <td>0.154804</td>\n",
       "      <td>-0.960454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.950012</td>\n",
       "      <td>0.120686</td>\n",
       "      <td>-0.960468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.674336</td>\n",
       "      <td>0.142392</td>\n",
       "      <td>-0.964854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.863774</td>\n",
       "      <td>0.106579</td>\n",
       "      <td>-0.958275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>14.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.957521</td>\n",
       "      <td>0.079111</td>\n",
       "      <td>-0.956082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>19.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.695018</td>\n",
       "      <td>0.095213</td>\n",
       "      <td>-0.960483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.684442</td>\n",
       "      <td>0.147520</td>\n",
       "      <td>-0.960483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.592116</td>\n",
       "      <td>0.081179</td>\n",
       "      <td>-0.958290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.614798</td>\n",
       "      <td>0.076255</td>\n",
       "      <td>-0.960483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.776738</td>\n",
       "      <td>0.089624</td>\n",
       "      <td>-0.962676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.514772</td>\n",
       "      <td>0.092214</td>\n",
       "      <td>-0.960483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>19.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.949783</td>\n",
       "      <td>0.083983</td>\n",
       "      <td>-0.949503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.926121</td>\n",
       "      <td>0.112477</td>\n",
       "      <td>-0.949503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.570990</td>\n",
       "      <td>0.064663</td>\n",
       "      <td>-0.958290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.884549</td>\n",
       "      <td>0.042766</td>\n",
       "      <td>-0.953904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.548302</td>\n",
       "      <td>0.184028</td>\n",
       "      <td>-0.960468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.910278</td>\n",
       "      <td>0.133006</td>\n",
       "      <td>-0.958261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.532501</td>\n",
       "      <td>0.091771</td>\n",
       "      <td>-0.960483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.813570</td>\n",
       "      <td>0.189043</td>\n",
       "      <td>-0.958290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.766711</td>\n",
       "      <td>0.154057</td>\n",
       "      <td>-0.962676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.647222</td>\n",
       "      <td>0.162628</td>\n",
       "      <td>-0.962661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>16.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.790628</td>\n",
       "      <td>0.055192</td>\n",
       "      <td>-0.951696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>17.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.813718</td>\n",
       "      <td>0.011072</td>\n",
       "      <td>-0.940717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>14.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.721848</td>\n",
       "      <td>0.126014</td>\n",
       "      <td>-0.960468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.653973</td>\n",
       "      <td>0.171861</td>\n",
       "      <td>-0.960483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>11.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.832993</td>\n",
       "      <td>0.138894</td>\n",
       "      <td>-0.960468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>17.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.755943</td>\n",
       "      <td>0.040738</td>\n",
       "      <td>-0.949503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.641440</td>\n",
       "      <td>0.173422</td>\n",
       "      <td>-0.962676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>14.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.624455</td>\n",
       "      <td>0.175341</td>\n",
       "      <td>-0.962661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.722442</td>\n",
       "      <td>0.192232</td>\n",
       "      <td>-0.962676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.724079</td>\n",
       "      <td>0.195664</td>\n",
       "      <td>-0.962676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>20.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.675185</td>\n",
       "      <td>0.195456</td>\n",
       "      <td>-0.962676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>20.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.671895</td>\n",
       "      <td>0.143404</td>\n",
       "      <td>-0.953889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.992730</td>\n",
       "      <td>0.161850</td>\n",
       "      <td>-0.953889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>18.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.591854</td>\n",
       "      <td>0.115272</td>\n",
       "      <td>-0.956097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.700742</td>\n",
       "      <td>0.126307</td>\n",
       "      <td>-0.958290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.734703</td>\n",
       "      <td>0.199356</td>\n",
       "      <td>-0.960483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>20.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.558528</td>\n",
       "      <td>0.102859</td>\n",
       "      <td>-0.953889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>18.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.849342</td>\n",
       "      <td>0.183067</td>\n",
       "      <td>-0.960483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.783361</td>\n",
       "      <td>0.015392</td>\n",
       "      <td>-0.936331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>19.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.619606</td>\n",
       "      <td>0.153129</td>\n",
       "      <td>-0.962661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.770935</td>\n",
       "      <td>0.160257</td>\n",
       "      <td>-0.960483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>11.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.879004</td>\n",
       "      <td>0.102915</td>\n",
       "      <td>-0.953875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.643013</td>\n",
       "      <td>0.136868</td>\n",
       "      <td>-0.953904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>16.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.703787</td>\n",
       "      <td>0.147665</td>\n",
       "      <td>-0.962676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>17.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.516999</td>\n",
       "      <td>0.067299</td>\n",
       "      <td>-0.958290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>15.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.696660</td>\n",
       "      <td>0.117622</td>\n",
       "      <td>-0.960483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>18.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.602416</td>\n",
       "      <td>0.146912</td>\n",
       "      <td>-0.960468</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    max_depth  min_child_weight  colsample_bytree  learning_rate    losses\n",
       "0        19.0               2.0          0.585235       0.033688 -0.947296\n",
       "1         5.0               2.0          0.727186       0.105956 -0.958275\n",
       "2         6.0               2.0          0.959945       0.154804 -0.960454\n",
       "3         6.0               2.0          0.950012       0.120686 -0.960468\n",
       "4        16.0               2.0          0.674336       0.142392 -0.964854\n",
       "5         8.0               2.0          0.863774       0.106579 -0.958275\n",
       "6        14.0               2.0          0.957521       0.079111 -0.956082\n",
       "7        19.0               2.0          0.695018       0.095213 -0.960483\n",
       "8         9.0               2.0          0.684442       0.147520 -0.960483\n",
       "9         8.0               1.0          0.592116       0.081179 -0.958290\n",
       "10        6.0               2.0          0.614798       0.076255 -0.960483\n",
       "11        7.0               2.0          0.776738       0.089624 -0.962676\n",
       "12        8.0               2.0          0.514772       0.092214 -0.960483\n",
       "13       19.0               1.0          0.949783       0.083983 -0.949503\n",
       "14       10.0               1.0          0.926121       0.112477 -0.949503\n",
       "15        6.0               2.0          0.570990       0.064663 -0.958290\n",
       "16        7.0               2.0          0.884549       0.042766 -0.953904\n",
       "17       18.0               2.0          0.548302       0.184028 -0.960468\n",
       "18        6.0               2.0          0.910278       0.133006 -0.958261\n",
       "19        9.0               2.0          0.532501       0.091771 -0.960483\n",
       "20       15.0               1.0          0.813570       0.189043 -0.958290\n",
       "21       12.0               1.0          0.766711       0.154057 -0.962676\n",
       "22       12.0               1.0          0.647222       0.162628 -0.962661\n",
       "23       16.0               2.0          0.790628       0.055192 -0.951696\n",
       "24       17.0               2.0          0.813718       0.011072 -0.940717\n",
       "25       14.0               2.0          0.721848       0.126014 -0.960468\n",
       "26       12.0               1.0          0.653973       0.171861 -0.960483\n",
       "27       11.0               2.0          0.832993       0.138894 -0.960468\n",
       "28       17.0               2.0          0.755943       0.040738 -0.949503\n",
       "29       13.0               1.0          0.641440       0.173422 -0.962676\n",
       "30       14.0               1.0          0.624455       0.175341 -0.962661\n",
       "31       13.0               1.0          0.722442       0.192232 -0.962676\n",
       "32       16.0               1.0          0.724079       0.195664 -0.962676\n",
       "33       20.0               1.0          0.675185       0.195456 -0.962676\n",
       "34       20.0               1.0          0.671895       0.143404 -0.953889\n",
       "35       16.0               1.0          0.992730       0.161850 -0.953889\n",
       "36       18.0               1.0          0.591854       0.115272 -0.956097\n",
       "37       13.0               1.0          0.700742       0.126307 -0.958290\n",
       "38       15.0               1.0          0.734703       0.199356 -0.960483\n",
       "39       20.0               2.0          0.558528       0.102859 -0.953889\n",
       "40       18.0               1.0          0.849342       0.183067 -0.960483\n",
       "41       10.0               2.0          0.783361       0.015392 -0.936331\n",
       "42       19.0               2.0          0.619606       0.153129 -0.962661\n",
       "43       15.0               1.0          0.770935       0.160257 -0.960483\n",
       "44       11.0               2.0          0.879004       0.102915 -0.953875\n",
       "45       11.0               1.0          0.643013       0.136868 -0.953904\n",
       "46       16.0               2.0          0.703787       0.147665 -0.962676\n",
       "47       17.0               2.0          0.516999       0.067299 -0.958290\n",
       "48       15.0               2.0          0.696660       0.117622 -0.960483\n",
       "49       18.0               2.0          0.602416       0.146912 -0.960468"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses = [loss_dict['loss'] for loss_dict in trial_val.results]\n",
    "result_df = pd.DataFrame({'max_depth': trial_val.vals['max_depth'],\n",
    "                          'min_child_weight': trial_val.vals['min_child_weight'],\n",
    "                          'colsample_bytree': trial_val.vals['colsample_bytree'],\n",
    "                          'learning_rate': trial_val.vals['learning_rate'],\n",
    "                          'losses': losses\n",
    "                         }\n",
    "                        )\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013114eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
