### 분류

**분류 알고리즘**은 주어진 데이터 포인트를 미리 정의된 범주 또는 클래스로 분류하는 머신 러닝 알고리즘입니다. 이 알고리즘은 이진 분류(두 개의 클래스)와 다중 클래스 분류(세 개 이상의 클래스) 문제를 해결하는 데 사용됩니다.

### **분류 알고리즘의 정의**

분류 알고리즘은 입력 데이터를 분석하여 해당 데이터가 속할 클래스 또는 범주를 예측합니다. 이를 통해 분류 모델은 학습 데이터에서 패턴을 학습한 다음, 새로운 데이터에 대해 해당 데이터가 속하는 클래스를 예측할 수 있습니다.

### **주요 분류 알고리즘의 종류**

1. **로지스틱 회귀 (Logistic Regression)**

   - 선형 모델을 기반으로 하며, 이진 분류 문제에 주로 사용됩니다.
   - 출력은 0과 1 사이의 확률값이며, 특정 임계값을 기준으로 클래스를 예측합니다.

2. **K-최근접 이웃 알고리즘 (K-Nearest Neighbors, KNN)**

   - 새로운 데이터 포인트가 주어졌을 때, 가장 가까운 K개의 데이터 포인트의 클래스를 참고하여 해당 데이터의 클래스를 결정합니다.
   - 비모수(non-parametric) 방법으로, 데이터의 분포에 대한 가정을 하지 않습니다.

3. **서포트 벡터 머신 (Support Vector Machine, SVM)**

   - 데이터 포인트를 두 클래스 간의 가장 큰 경계(margin)를 가지는 초평면으로 분류합니다.
   - 선형적으로 구분되지 않는 데이터를 처리하기 위해 커널 트릭을 사용할 수 있습니다.

4. **나이브 베이즈 (Naive Bayes)**

   - 베이즈 정리를 기반으로 한 확률적 분류 알고리즘입니다.
   - 모든 특징이 독립적이라는 가정을 하며, 텍스트 분류와 같은 작업에 자주 사용됩니다.

5. **의사결정나무 (Decision Tree)**

   - 데이터의 특징에 따라 의사결정을 내리는 트리 구조를 기반으로 분류를 수행합니다.
   - 각 노드는 특징의 값을 기준으로 데이터를 분할하며, 최종 리프 노드는 예측된 클래스를 나타냅니다.

6. **랜덤 포레스트 (Random Forest)**

   - 다수의 의사결정나무를 앙상블하여 예측을 수행하는 알고리즘입니다.
   - 과적합을 방지하고 더 높은 예측 성능을 제공할 수 있습니다.

7. **신경망 (Neural Networks)**

   - 입력 데이터를 여러 계층의 뉴런으로 전달하여 복잡한 패턴을 학습합니다.
   - 다층 퍼셉트론(MLP)은 가장 기본적인 형태의 신경망입니다.

8. **그래디언트 부스팅 머신 (Gradient Boosting Machine, GBM)**

   - 이전 모델의 오류를 보완하는 방식으로 여러 약한 학습기를 결합하는 앙상블 방법입니다.
   - XGBoost, LightGBM, CatBoost와 같은 변형된 모델들이 있습니다.

9. **최근접 메모리 기반 모델 (Nearest Centroid, NC)**
   - 각 클래스의 중심을 기준으로 새로운 데이터를 분류하는 간단한 알고리즘입니다.
   - KNN과 유사하지만, K 대신 중심점을 사용합니다.

### **선택 기준**

각 분류 알고리즘은 데이터의 특성, 크기, 복잡성에 따라 성능이 다릅니다. 따라서 데이터에 적합한 알고리즘을 선택하기 위해 여러 알고리즘을 테스트해 보고, 정확도, 재현율, 정밀도 등의 평가 지표를 기준으로 모델을 선택하는 것이 중요합니다.

### 결정트리 및 앙상블

- 결정트리는 간편함. 과적합의 문제
- 앙상블 모델에서는 오히려 장점이 된다 약한학습기를 결합해서 모델을 결정하는데 결정트리가 괜찮은 약한학습기이다.
  **결정트리 알고리즘 원리**
  결정 트리(Decision Tree) 알고리즘은 데이터의 특징(feature)에 따라 의사결정을 내리는 트리 구조를 기반으로 한 분류 및 회귀 알고리즘입니다. 이 알고리즘은 데이터를 여러 노드로 분할하여 최종적으로 데이터를 특정 클래스(또는 값)로 예측합니다.

