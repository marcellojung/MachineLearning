# 부스팅(Boosting) 알고리즘
 약한 학습기(weak learner)를 순차적으로 학습시키고, 이를 결합하여 강한 학습기(strong learner)를 만드는 앙상블 학습 방법입니다. 부스팅의 핵심 아이디어는 이전 단계에서 잘못 예측된 샘플에 가중치를 더 부여하여, 다음 학습 단계에서 이러한 오류를 교정하는 것입니다. 이를 통해 모델의 예측 성능을 점진적으로 향상시킬 수 있습니다.

## **부스팅 알고리즘의 주요 개념**

1. **순차적 학습**: 부스팅은 약한 학습기를 순차적으로 학습시킵니다. 각 단계에서 이전 단계의 예측 오류를 보완하는 학습기를 추가합니다.

2. **가중치 조정**: 각 학습 단계에서는 이전 단계에서 잘못 예측된 샘플에 더 높은 가중치를 부여합니다. 이를 통해 모델이 어려운 샘플에 더 집중하게 만듭니다.

3. **결합 예측**: 최종 예측은 모든 학습기의 예측을 결합하여 만듭니다. 일반적으로 가중치를 부여한 합을 취하거나, 특정 규칙을 적용하여 결합합니다.

## **주요 부스팅 알고리즘**

#### 1. **AdaBoost (Adaptive Boosting)**
   - **개념**: AdaBoost는 부스팅 알고리즘 중 가장 기본적인 형태로, 각 학습 단계에서 잘못 예측된 샘플의 가중치를 증가시켜 다음 학습기가 이러한 샘플을 더 잘 예측하도록 합니다.
   - **작동 방식**:
     1. 모든 샘플에 동일한 가중치를 부여하고, 약한 학습기를 학습시킵니다.
     2. 약한 학습기의 예측 오류에 따라 샘플의 가중치를 조정합니다.
     3. 가중치를 조정한 데이터로 다음 학습기를 학습시킵니다.
     4. 최종 예측은 모든 학습기의 예측을 가중 평균하여 결합합니다.
   - **장점**: 비교적 간단하며, 과적합을 잘 방지할 수 있습니다.
   - **단점**: 노이즈 데이터에 민감할 수 있으며, 데이터 불균형 문제에서 성능이 떨어질 수 있습니다.

#### 2. **Gradient Boosting Machine (GBM)**
   - **개념**: GBM은 부스팅 알고리즘의 발전된 형태로, 각 학습 단계에서 이전 모델의 잔차(residuals, 실제 값과 예측 값의 차이)를 줄이는 방향으로 새로운 모델을 학습시킵니다.
   경사하강법을 이용해서 가중치를 업데이트합니다. 
   - **작동 방식**:
     1. 초기 모델을 학습시켜 예측값을 생성합니다.
     2. 잔차를 계산하고, 이 잔차를 예측하는 새로운 모델을 학습합니다.
     3. 새로운 모델의 예측을 이전 모델의 예측과 결합하여 잔차를 줄입니다.
     4. 이 과정을 반복하여 최종 모델을 생성합니다.
   - **장점**: 매우 강력한 성능을 제공하며, 다양한 유형의 데이터에서 높은 정확도를 보입니다.
   - **단점**: 학습 속도가 느리고, 하이퍼파라미터 튜닝이 중요합니다. 과적합의 위험이 있을 수 있습니다.

#### 3. **XGBoost (Extreme Gradient Boosting)**
   - **개념**: XGBoost는 GBM을 개선한 알고리즘으로, 학습 속도와 성능을 향상시키기 위해 다양한 기술적 최적화를 적용한 부스팅 알고리즘입니다.
   - **주요 특징**:
     - **Regularization**: 정규화를 통해 과적합을 방지합니다.
     - **Sparsity Awareness**: 데이터의 희소성을 인식하여 효율적으로 처리합니다.
     - **Parallel Computing**: 병렬 처리를 통해 학습 속도를 크게 향상시킵니다.
   - **장점**: 높은 성능과 효율성, 과적합 방지.
   - **단점**: 복잡한 모델 구조로 인해 이해하기 어려울 수 있으며, 하이퍼파라미터 튜닝이 복잡할 수 있습니다.

#### 4. **LightGBM**
   - **개념**: LightGBM은 XGBoost와 유사하지만, 큰 데이터셋을 빠르게 처리할 수 있도록 설계된 부스팅 알고리즘입니다.
   - **주요 특징**:
     - **Leaf-wise Growth**: 기존의 레벨-wise 트리 성장 방식 대신 리프-wise 트리 성장 방식을 사용하여 더 깊고 효율적인 트리를 만듭니다.
     - **Histogram-based**: 데이터 양이 많은 경우에도 빠른 학습이 가능하도록 히스토그램 기반 방법을 사용합니다.
   - **장점**: 매우 빠른 학습 속도와 낮은 메모리 사용량.
   - **단점**: 데이터가 적거나 불균형할 때 성능이 떨어질 수 있습니다.

#### 5. **CatBoost**
   - **개념**: CatBoost는 범주형 데이터 처리에 최적화된 부스팅 알고리즘입니다.
   - **주요 특징**:
     - **Categorical Feature Handling**: 범주형 변수를 효과적으로 처리할 수 있는 기능을 내장하고 있습니다.
     - **Ordered Boosting**: 순서에 민감한 부스팅 방식을 사용하여 과적합을 방지합니다.
   - **장점**: 범주형 데이터가 많은 데이터셋에서 뛰어난 성능을 발휘합니다.
   - **단점**: 다른 부스팅 알고리즘에 비해 학습 속도가 느릴 수 있습니다.

### **부스팅 알고리즘의 특징 요약**

- **점진적 학습**: 학습기가 순차적으로 학습하고, 이전 단계의 오류를 수정하며 학습을 진행.
- **강력한 성능**: 복잡한 데이터에서도 높은 예측 정확도를 보임.
- **과적합 위험**: 잘못된 하이퍼파라미터 설정 시 과적합이 발생할 수 있음.
- **느린 학습 속도**: 학습 속도가 비교적 느리지만, 병렬 처리를 통해 개선 가능.

부스팅 알고리즘은 데이터 분석과 머신러닝 모델링에서 매우 중요한 도구이며, 다양한 문제에서 강력한 성능을 제공합니다. 모델의 성능을 극대화하기 위해서는 하이퍼파라미터 튜닝과 데이터 특성에 맞는 알고리즘 선택이 필수적입니다.


Gradient Boosting Machine (GBM)은 강력한 예측 성능을 제공하는 앙상블 학습 알고리즘입니다. GBM의 성능을 최적화하려면 하이퍼파라미터를 적절하게 조정하는 것이 중요합니다. 아래는 GBM에서 주요 하이퍼파라미터와 그 역할에 대한 설명입니다.

### 1. **`n_estimators` (트리의 개수)**
   - **설명**: GBM이 생성하는 결정 트리의 수를 지정합니다. 트리의 개수가 많을수록 모델이 복잡해지고, 학습 시간이 증가하지만, 성능이 개선될 수 있습니다.
   - **기본값**: 보통 100.
   - **튜닝 팁**: 일반적으로 큰 값(200~500)을 설정하고, `learning_rate`를 낮추어 성능을 개선합니다.

### 2. **`learning_rate` (학습률)**
   - **설명**: 각 트리가 기여하는 정도를 조절하는 매개변수입니다. 낮은 학습률은 더 많은 트리를 필요로 하지만, 일반적으로 성능이 더 좋습니다.
   - **기본값**: 0.1.
   - **튜닝 팁**: `n_estimators`와 함께 조정하여 성능을 최적화합니다. 보통 0.01, 0.05, 0.1 등의 값을 시도해 볼 수 있습니다.

### 3. **`max_depth` (트리의 최대 깊이)**
   - **설명**: 각 결정 트리의 최대 깊이를 지정합니다. 트리가 깊을수록 모델이 더 복잡해지고, 과적합할 위험이 있습니다.
   - **기본값**: 3.
   - **튜닝 팁**: 작은 값을 시도하여 과적합을 방지할 수 있습니다. 보통 3에서 5 사이의 값을 자주 사용합니다.

### 4. **`min_samples_split` (노드를 분할하기 위한 최소 샘플 수)**
   - **설명**: 노드를 분할하기 위해 필요한 최소 샘플 수를 지정합니다. 이 값이 크면 트리가 덜 분할되어 덜 복잡해집니다.
   - **기본값**: 2.
   - **튜닝 팁**: 데이터 양과 노이즈에 따라 조정하며, 보통 10 이상의 값을 사용하여 과적합을 방지할 수 있습니다.

### 5. **`min_samples_leaf` (리프 노드에 있어야 하는 최소 샘플 수)**
   - **설명**: 리프 노드에 있어야 하는 최소 샘플 수를 지정합니다. 값이 클수록 리프 노드가 더 많이 결합되어 모델이 덜 복잡해집니다.
   - **기본값**: 1.
   - **튜닝 팁**: 1에서 5 사이의 값을 자주 사용하며, 데이터셋의 크기에 따라 조정합니다.

### 6. **`subsample` (샘플링 비율)**
   - **설명**: 각 트리를 학습할 때 사용하는 데이터 샘플의 비율을 지정합니다. 값이 1.0보다 작으면 부분 샘플링을 수행하며, 이는 모델의 다양성을 증가시켜 과적합을 줄일 수 있습니다.
   - **기본값**: 1.0 (전체 데이터를 사용).
   - **튜닝 팁**: 보통 0.7에서 0.9 사이의 값을 사용합니다. 너무 작은 값은 모델 성능을 저하시킬 수 있습니다.

### 7. **`max_features` (최대 특징 수)**
   - **설명**: 각 노드를 분할할 때 고려할 최대 특징 수를 지정합니다. `auto`, `sqrt`, `log2` 또는 특정 정수 값으로 설정할 수 있습니다.
   - **기본값**: None (모든 특징 사용).
   - **튜닝 팁**: `sqrt` 또는 `log2`를 시도하여 성능을 개선할 수 있습니다. 적은 특징 수를 사용할수록 모델의 다양성이 증가합니다.

### 8. **`loss` (손실 함수)**
   - **설명**: 모델이 최소화하려는 손실 함수를 지정합니다. 분류 문제에서는 `deviance` (logistic loss)를, 회귀 문제에서는 `ls` (least squares)를 기본으로 사용합니다.
   - **기본값**: `deviance` (분류), `ls` (회귀).
   - **튜닝 팁**: 문제 유형에 따라 적절한 손실 함수를 선택합니다. 예를 들어, 회귀 문제에서 `huber` 손실을 사용하여 이상치에 덜 민감하게 만들 수 있습니다.

### 9. **`random_state` (랜덤 시드)**
   - **설명**: 모델의 랜덤성을 제어하여 결과를 재현 가능하게 합니다.
   - **기본값**: None.
   - **튜닝 팁**: 특정 값을 설정하여 실험을 반복할 때 동일한 결과를 얻을 수 있습니다.

### 10. **`alpha` (허브 손실 함수의 민감도 조절)**
   - **설명**: `loss`를 `huber`로 설정했을 때 사용하는 매개변수로, 허브 손실 함수의 민감도를 조정합니다.
   - **기본값**: 0.9.
   - **튜닝 팁**: 회귀 문제에서 이상치에 대한 민감도를 조절할 때 사용합니다.

### 11. **`warm_start`**
   - **설명**: `True`로 설정하면 이전에 학습된 모델의 결과를 바탕으로 추가 학습을 진행할 수 있습니다.
   - **기본값**: False.
   - **튜닝 팁**: 모델을 점진적으로 학습시키고 싶을 때 사용합니다.

### **하이퍼파라미터 튜닝 전략**

- **Grid Search**: 모든 조합을 시도해보는 방식으로, 가장 성능이 좋은 하이퍼파라미터를 찾을 수 있습니다.
- **Random Search**: 무작위로 일부 조합을 시도하여 효율적으로 최적의 하이퍼파라미터를 찾는 방법입니다.
- **Bayesian Optimization**: 확률적 모델을 사용해 가장 유망한 하이퍼파라미터 조합을 탐색하는 방법입니다.

GBM에서 하이퍼파라미터 튜닝은 모델 성능에 큰 영향을 미치므로, 데이터셋의 특성과 문제에 맞게 신중하게 조정해야 합니다. 최적의 하이퍼파라미터를 찾으면 GBM의 성능을 극대화할 수 있습니다.