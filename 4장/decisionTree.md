# 결정 트리
결정 트리는 루트 노드(root node), 내부 노드(internal nodes), 가지(branches), 그리고 **리프 노드(leaf nodes)**로 구성됩니다.

루트 노드: 트리의 최상위 노드로, 첫 번째 특징을 기준으로 데이터를 분할합니다.
내부 노드: 각 분할을 나타내며, 특정 특징의 조건에 따라 데이터를 분리합니다.
가지: 특징의 조건에 따른 데이터 흐름을 나타내며, 다음 노드로 이어집니다.
리프 노드: 최종 결정이 이루어지는 곳으로, 여기서 데이터가 특정 클래스 또는 값을 갖게 됩니다.

결정 트리는 모든 가능한 분할을 평가하여 정보 이득(엔트로피)이 최대가 되는 특징을 선택합니다. 정보 이득이 최대인 분할은 불확실성을 가장 많이 줄이는 특징을 의미하며, 이를 통해 데이터가 더 잘 분리됩니다.
이 과정을 루트 노드부터 반복하여 트리를 구성하며, 최종적으로 모든 데이터가 잘 분류되도록 합니다.

** 예시** 
 - 예를 들어, 주어진 데이터셋에서 엔트로피를 계산하고 특정 특징으로 분할한 후의 정보 이득을 계산해봅니다:
분할 전 엔트로피 계산: 만약 데이터셋에 두 개의 클래스가 있으며, 이 중 하나의 클래스에 속하는 데이터가 전체의 80%라면,
분할 후 엔트로피 계산: 특정 특징으로 데이터를 분할했을 때, 각 분할된 그룹의 엔트로피와 그 비율을 사용해 엔트로피를 계산하고, 정보 이득을 구합니다.
이렇게 하여 결정 트리는 데이터를 순차적으로 분할하며, 최적의 분할을 찾아 트리를 완성합니다.

**엔트로피 결론**

엔트로피는 결정 트리에서 불확실성을 측정하여 최적의 분할을 결정하는 데 중요한 역할을 합니다. 정보 이득을 통해 트리가 데이터를 어떻게 분할할지를 선택하게 되며, 이를 통해 최종적으로 데이터가 잘 분류된 트리가 구축됩니다.


## 결정트리 하이퍼파라미터
결정 트리(Decision Tree) 알고리즘의 성능을 최적화하기 위해서는 여러 하이퍼파라미터를 적절하게 설정하는 것이 중요합니다. 이러한 하이퍼파라미터는 트리의 복잡도를 조절하고, 과적합(overfitting)이나 과소적합(underfitting)을 방지하는 데 중요한 역할을 합니다. 아래는 결정 트리의 주요 하이퍼파라미터와 그 역할에 대한 설명입니다.

### 1. **`max_depth` (트리의 최대 깊이)**
   - 트리의 최대 깊이를 제한합니다.
   - 트리의 깊이가 깊어질수록 더 복잡한 모델이 생성되며, 과적합의 위험이 커집니다.
   - 반대로, 깊이가 너무 얕으면 트리가 충분히 학습하지 못해 과소적합이 발생할 수 있습니다.
   - 일반적으로 적절한 `max_depth`를 설정하여 트리의 복잡도를 제어합니다.

### 2. **`min_samples_split` (노드를 분할하기 위한 최소 샘플 수)**
   - 노드를 분할하기 위해 필요한 최소 샘플 수를 설정합니다.
   - 이 값이 크면 분할이 적게 발생하여 트리가 덜 복잡해지고, 과적합을 방지할 수 있습니다.
   - 기본값은 `2`이며, 최소한 `2`개의 샘플이 있어야 노드를 분할할 수 있습니다.

### 3. **`min_samples_leaf` (리프 노드에 있어야 하는 최소 샘플 수)**
   - 리프 노드에 포함되어야 하는 최소 샘플 수를 지정합니다.
   - 이 값이 크면 리프 노드가 적게 생성되어 트리의 복잡도가 낮아집니다.
   - 과적합을 방지하는 데 유용하며, 샘플 수가 적은 노드를 방지합니다.

### 4. **`max_features` (최대 특징 수)**
   - 분할을 고려할 때 사용할 최대 특징 수를 지정합니다.
   - 값으로는 `None` (모든 특징 사용), `sqrt` (전체 특징의 제곱근), `log2` (전체 특징의 로그), 또는 특정 정수 값을 지정할 수 있습니다.
   - `max_features`를 작게 설정하면 모델의 분산이 줄어들어 과적합을 방지할 수 있습니다. ->분할규칙을 굉장히 깊이 할 수 있기 때문에...작게

### 5. **`criterion` (분할 품질 기준)**
   - 노드 분할 시 사용할 기준을 설정합니다.
   - **`gini`** (기본값): 지니 불순도를 최소화하는 방향으로 분할합니다.
   - **`entropy`**: 정보 이득을 최대화하는 방향으로 분할합니다.
   - 두 기준 모두 분할을 평가하는 데 사용되며, 특정 데이터셋에서는 하나가 다른 것보다 더 나은 성능을 보일 수 있습니다.

### 6. **`max_leaf_nodes` (리프 노드의 최대 수)**
   - 리프 노드의 최대 수를 제한합니다.
   - 이 값이 설정되면 트리의 리프 노드 수가 지정된 값 이상 생성되지 않도록 트리 성장이 중지됩니다.
   - 이를 통해 트리의 복잡도를 제어하고, 과적합을 방지할 수 있습니다.

### 7. **`min_weight_fraction_leaf` (리프 노드의 최소 가중치 비율)**
   - 전체 샘플에 대한 가중치 합의 최소 비율을 설정합니다.
   - 가중치가 작은 데이터를 가진 리프 노드의 생성을 방지하여 모델의 복잡도를 줄입니다.

### 8. **`splitter` (분할 전략)**
   - 노드를 분할할 때 사용할 전략을 지정합니다.
   - **`best`**: 가능한 최적의 분할을 선택합니다.
   - **`random`**: 무작위로 분할을 선택하여 트리를 만듭니다.
   - `random`은 트리의 다양성을 증가시키며, 특히 앙상블 모델에서 유용할 수 있습니다.

### 9. **`class_weight` (클래스 가중치)**
   - 클래스의 불균형 문제를 해결하기 위해 사용됩니다.
   - **`balanced`**: 자동으로 클래스 비율에 반비례하여 가중치를 설정합니다.
   - 특정 클래스에 더 높은 가중치를 부여하여 모델이 해당 클래스를 더 잘 예측하도록 할 수 있습니다.

### 10. **`min_impurity_decrease` (최소 불순도 감소)**
   - 노드를 분할할 때 불순도가 지정된 값 이상으로 감소하는 경우에만 분할이 이루어지도록 합니다.
   - 이 값을 높게 설정하면 트리가 더 적게 분할되어 과적합을 방지할 수 있습니다.

### **하이퍼파라미터 튜닝 방법**
하이퍼파라미터를 최적화하기 위해 다음과 같은 방법을 사용할 수 있습니다:
- **그리드 서치(Grid Search)**: 미리 정의된 하이퍼파라미터 값의 모든 조합을 시도하여 최적의 값을 찾습니다.
- **랜덤 서치(Random Search)**: 하이퍼파라미터 공간에서 임의의 값을 선택하여 최적의 값을 찾습니다.
- **베이지안 최적화(Bayesian Optimization)**: 이전 평가 결과를 바탕으로 하이퍼파라미터를 점진적으로 최적화합니다.

결정 트리 모델의 성능은 데이터셋에 따라 달라질 수 있으므로, 다양한 하이퍼파라미터를 실험해 보고 최적의 조합을 찾는 것이 중요합니다.