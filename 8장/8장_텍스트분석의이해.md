# 텍스트 분석의 이해

- 텍스트 분류
- 감성 분석 : 텍스트에 나타내는 감정/판단.믿음/으;ㅣ견 등 주관적요소를 분석하는 기법 sentiment
- 텍스트 요약
- 텍스트 군집화와 유사도 측정

# 텍스트 분석 머신러닝 수행 프로세스

- 데이터 수집: 텍스트 데이터를 수집합니다. 예를 들어, 웹 스크래핑, API, 데이터베이스 등에서 데이터를 가져옵니다.
- 데이터 전처리:

      - 텍스트 정제: 불필요한 문자, 공백, 특수 문자 등을 제거합니다.
      - 토큰화: 텍스트를 단어 또는 문장 단위로 분리합니다.
      - 불용어 제거: 분석에 필요 없는 일반적인 단어(예: "the", "is")를 제거합니다.
      -   어간 추출 및 표제어 추출: 단어의 기본 형태로 변환합니다.
      - 특징 추출: 벡터화: 텍스트 데이터를 수치 데이터로 변환합니다. 예를 들어, TF-IDF, Word2Vec, BERT 등을 사용합니다.

  데이터 분할: 데이터를 학습용(train)과 테스트용(test)으로 분할합니다.

- 모델 학습: 머신러닝 알고리즘을 사용하여 모델을 학습합니다. 예를 들어, Naive Bayes, SVM, LSTM 등을 사용합니다.

- 모델 평가: 테스트 데이터를 사용하여 모델의 성능을 평가합니다. 예를 들어, 정확도, 정밀도, 재현율, F1 점수 등을 사용합니다.

- 모델 튜닝: 하이퍼파라미터 튜닝, 교차 검증 등을 통해 모델을 최적화합니다.

- 모델 배포: 최종 모델을 배포하여 실제 환경에서 사용합니다.

- 모니터링 및 유지보수: 모델의 성능을 지속적으로 모니터링하고 필요에 따라 업데이트합니다.

## 파이썬 기반의 NLP, 텍스트 분석 패키ㅣ

- NLTK : academy style
- Gensim : topic modeling (word2vec)
- Spacy : 최근 몇년 간 많이 씀

## 텍스트 전처리 : 텍스트 정규화

- 클렌징 : 불필요한 문자 및 기호를 사전에 제거. 태그나 특정기호
- 토큰화 : 문장, 단어 토큰화
  - N-gram : 문맥적 의미가 무시되는 토큰화 단계를 보안하기 위해 n개의 단어를 하나의 토큰화 단위로 분리하는 것. n개 단위 크기 위ㅣㄴ도우를 만들어 문장의 처음부터 오른쪽으로 움직이면서 토큰화를 한다.
    [('The', 'Matrix'), ('Matrix', 'is'), ('is', 'everywhere'), ('everywhere', 'its'), ('its', 'all'), ('all', 'around'), ('around', 'us'), ('us', ','), (',', 'here'), ('here', 'even'), ('even', 'in'), ('in', 'this'), ('this', 'room'), ('room', '.')]
- 필터링.스톱워드 제거 . 절차 수정 : a the, will,
- stemming, Lemmatization : 어근단어 추출(영어는 3인칭이나, 과거형이나. Lemmatization이 좀 더 정교하고 의미론적임

# 텍스트의 피처 벡터화 유형

- BOW : 개별단어를 feature로 만든다. 해당 단어의 횟수들을 벡터로. document Term Matrix로 만든다.
- Word2Vec : word Embedding : 100개 문서를 분석해서 개별단어를 문맥을 가지는 N차원 공간에 벡터로 표현 . 차원축의 포인트로 .
  Man, King의 관계 = woman, Queen 관계

## BOW : Bag of Words

- 문서가 가지는 모든 단어를 문맥, 순서를 무시하고 일괄적으로 단어에 대해 빈도값을 부여함.
- 문장1, 문장 2 단어에서 중복을 제거하고 각 단어에 고유한 위치 인덱스를 부여한다. 개별 단어에 대해 카운터를 한다.
- 쉽고 빠른 구축(워드 임베딩 보다), 예상보다 문서의 특징을 잘 나타내어 전통적으로 여러 분야에 활용도가 높음.
- 단점은 문맥의미(semantic context) 반영 문제, 희소 행렬 문제(머신러닝의 고질적 문제)
- 피처 벡터화 : M개의 문서가 있고, N개의 중복 단어가 있다면 MXN 행렬을 가진다.
- 피처 벡터화 유형1) 단순 카운트 기반의 벡터화
- 피처 벡터화 유형2) TF-IDF 벡터화 : 업무 상자주 사용되는 단어가 높은 값 부여. 이런 문제를 보완하기 위해 개발됨. 높은 가중치를 주되, 모든 문서에서 전반적으로 자주 나타내는 단어에 대해 패널티를 주는 방식으로 값을 부여함.
  - TF : term Frequency : 얼마나 나왔는지 (단어가)
  - DF : 해당 단어가 몇개의 문서에 나타났는지
  - IDF : DF의 역수 전체문서/DF
  - TF 도 높지만, DF 도 높으면

### BOW 구현 : countVectorizer를 이용한 피처 벡터화

- 사전데이터 가공 : 모든 문자를 소문자(default lowercase=true\_
- 토큰화 : n_gram_range를 반영
- 텍스트 정규화 : stop words 필터링만 수행
- 피처 벡터화 : max_df, min_df, max_features의 파라미터를 반영하여 토큰된 단어들을 feature extraction 후 vectorization 적용
  from sklearn.feature_extraction.text import CountVectorizer

'Count Vectorization으로 feature extraction 변환 수행.
cnt_vect = CountVectorizer()
cnt_vect.fit(text)

params: {'analyzer': 'word', 'binary': False, 'decode_error': 'strict', 'dtype': <class 'numpy.int64'>, 'encoding': 'utf-8', 'input': 'content', 'lowercase': True, 'max_df': 1.0, 'max_features': None, 'min_df': 1, 'ngram_range': (1, 1), 'preprocessor': None, 'stop_words': None, 'strip_accents': None, 'token_pattern': '(?u)\\b\\w\\w+\\b', 'tokenizer': None, 'vocabulary': None}

## 희소 행렬

- CSR Matrix : BOW의 벡토라이제이션 모델은 너무 많은 0의 ㄱ밧이 메모리 공간에 할당되어(모든 단어가 나오기 때문에) 너무 많은 시간, 공간이 소모된다
- 따라서 CSR 형식. 좌표형식 -> 0이 아닌 데이턱 값 배열, 행 과 열 위치 파악

dense3 = np.array([[0,0,1,0,0,5],
             [1,4,0,3,2,5],
             [0,6,0,3,0,0],
             [2,0,0,0,0,0],
             [0,0,0,7,0,8],
             [1,0,0,0,0,0]])

coo = sparse.coo_matrix(dense3)
csr = sparse.csr_matrix(dense3)

## 중요 : countVectorizer 와 TF-IDF VERCORIZER 차이
CountVectorizer와 TfidfVectorizer는 둘 다 텍스트 데이터를 수치화하여 머신러닝 모델에 사용할 수 있는 형태로 변환하는 도구입니다. 하지만, 이들은 데이터를 처리하는 방식에서 차이가 있습니다.

1. CountVectorizer
CountVectorizer는 텍스트 데이터를 단어의 빈도(frequency)로 변환합니다. 각 문서에서 등장하는 단어의 수를 세고, 이를 문서-단어 행렬로 변환합니다.

작동 원리: 각 문서에서 등장한 단어의 빈도를 세어 해당 단어가 등장한 횟수를 행렬로 변환합니다. 각 행은 문서를, 각 열은 단어를 나타내며, 셀 값은 해당 단어가 해당 문서에 몇 번 등장했는지를 의미합니다.
장점: 간단하고 직관적입니다. 단순한 빈도를 측정할 때 유용합니다.
단점: 단어가 문서 내에서 자주 등장한다고 해서 그 문서에 더 중요한 단어라고 할 수 없는데, 이를 고려하지 않습니다.
2. TfidfVectorizer
TfidfVectorizer는 단어의 빈도뿐만 아니라 문서 전체에서의 중요성도 고려합니다. 이를 위해 TF-IDF (Term Frequency-Inverse Document Frequency) 방식으로 단어의 가중치를 부여합니다.

작동 원리:
TF (Term Frequency): 해당 단어가 문서에서 등장한 빈도를 나타냅니다.
IDF (Inverse Document Frequency): 해당 단어가 다른 문서들에서 얼마나 등장하는지를 측정합니다. 많이 등장하는 단어는 중요도가 낮다고 판단되므로 낮은 가중치를 부여합니다.
두 값을 곱하여 문서에서 단어의 중요도를 결정합니다. 자주 등장하지만, 모든 문서에서 흔히 등장하지 않는 단어일수록 높은 가중치가 부여됩니다.
장점: 일반적으로 흔한 단어(예: "the", "is", "and" 같은 영어의 불용어)가 너무 큰 영향을 미치지 않도록 가중치를 조정하여 문서의 중요한 단어들을 잘 반영합니다.
단점: 계산이 CountVectorizer보다 더 복잡하고, 소규모 데이터에서는 빈도 기반 방법보다 더 적합하지 않을 수 있습니다.
정리
CountVectorizer: 단순히 단어의 등장 빈도만 고려.
TfidfVectorizer: 단어의 빈도뿐 아니라, 다른 문서에서의 중요성도 반영하여 단어의 가중치를 계산.
두 방법 중 어느 것이 더 적합한지는 분석하려는 데이터의 특성과 목적에 따라 다릅니다.
### 여기에 tfidvectorizer(n_gram과 stopwords 추가 시 더 성능이 좋아진다 )
 - 단, n_gram을 늘린다고 해서 성능이 맨날 좋아지지는 않는다!





