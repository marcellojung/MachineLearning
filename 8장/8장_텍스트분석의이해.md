# 텍스트 분석의 이해

- 텍스트 분류
- 감성 분석 : 텍스트에 나타내는 감정/판단.믿음/으;ㅣ견 등 주관적요소를 분석하는 기법 sentiment
- 텍스트 요약
- 텍스트 군집화와 유사도 측정

# 텍스트 분석 머신러닝 수행 프로세스

- 데이터 수집: 텍스트 데이터를 수집합니다. 예를 들어, 웹 스크래핑, API, 데이터베이스 등에서 데이터를 가져옵니다.
- 데이터 전처리:

      - 텍스트 정제: 불필요한 문자, 공백, 특수 문자 등을 제거합니다.
      - 토큰화: 텍스트를 단어 또는 문장 단위로 분리합니다.
      - 불용어 제거: 분석에 필요 없는 일반적인 단어(예: "the", "is")를 제거합니다.
      -   어간 추출 및 표제어 추출: 단어의 기본 형태로 변환합니다.
      - 특징 추출: 벡터화: 텍스트 데이터를 수치 데이터로 변환합니다. 예를 들어, TF-IDF, Word2Vec, BERT 등을 사용합니다.

  데이터 분할: 데이터를 학습용(train)과 테스트용(test)으로 분할합니다.

- 모델 학습: 머신러닝 알고리즘을 사용하여 모델을 학습합니다. 예를 들어, Naive Bayes, SVM, LSTM 등을 사용합니다.

- 모델 평가: 테스트 데이터를 사용하여 모델의 성능을 평가합니다. 예를 들어, 정확도, 정밀도, 재현율, F1 점수 등을 사용합니다.

- 모델 튜닝: 하이퍼파라미터 튜닝, 교차 검증 등을 통해 모델을 최적화합니다.

- 모델 배포: 최종 모델을 배포하여 실제 환경에서 사용합니다.

- 모니터링 및 유지보수: 모델의 성능을 지속적으로 모니터링하고 필요에 따라 업데이트합니다.

## 파이썬 기반의 NLP, 텍스트 분석 패키ㅣ

- NLTK : academy style
- Gensim : topic modeling (word2vec)
- Spacy : 최근 몇년 간 많이 씀

## 텍스트 전처리 : 텍스트 정규화

- 클렌징 : 불필요한 문자 및 기호를 사전에 제거. 태그나 특정기호
- 토큰화 : 문장, 단어 토큰화
  - N-gram : 문맥적 의미가 무시되는 토큰화 단계를 보안하기 위해 n개의 단어를 하나의 토큰화 단위로 분리하는 것. n개 단위 크기 위ㅣㄴ도우를 만들어 문장의 처음부터 오른쪽으로 움직이면서 토큰화를 한다.
    [('The', 'Matrix'), ('Matrix', 'is'), ('is', 'everywhere'), ('everywhere', 'its'), ('its', 'all'), ('all', 'around'), ('around', 'us'), ('us', ','), (',', 'here'), ('here', 'even'), ('even', 'in'), ('in', 'this'), ('this', 'room'), ('room', '.')]
- 필터링.스톱워드 제거 . 절차 수정 : a the, will,
- stemming, Lemmatization : 어근단어 추출(영어는 3인칭이나, 과거형이나. Lemmatization이 좀 더 정교하고 의미론적임

# 텍스트의 피처 벡터화 유형

- BOW : 개별단어를 feature로 만든다. 해당 단어의 횟수들을 벡터로. document Term Matrix로 만든다.
- Word2Vec : word Embedding : 100개 문서를 분석해서 개별단어를 문맥을 가지는 N차원 공간에 벡터로 표현 . 차원축의 포인트로 .
  Man, King의 관계 = woman, Queen 관계

## BOW : Bag of Words

- 문서가 가지는 모든 단어를 문맥, 순서를 무시하고 일괄적으로 단어에 대해 빈도값을 부여함.
- 문장1, 문장 2 단어에서 중복을 제거하고 각 단어에 고유한 위치 인덱스를 부여한다. 개별 단어에 대해 카운터를 한다.
- 쉽고 빠른 구축(워드 임베딩 보다), 예상보다 문서의 특징을 잘 나타내어 전통적으로 여러 분야에 활용도가 높음.
- 단점은 문맥의미(semantic context) 반영 문제, 희소 행렬 문제(머신러닝의 고질적 문제)
- 피처 벡터화 : M개의 문서가 있고, N개의 중복 단어가 있다면 MXN 행렬을 가진다.
- 피처 벡터화 유형1) 단순 카운트 기반의 벡터화
- 피처 벡터화 유형2) TF-IDF 벡터화 : 업무 상자주 사용되는 단어가 높은 값 부여. 이런 문제를 보완하기 위해 개발됨. 높은 가중치를 주되, 모든 문서에서 전반적으로 자주 나타내는 단어에 대해 패널티를 주는 방식으로 값을 부여함.
  - TF : term Frequency : 얼마나 나왔는지 (단어가)
  - DF : 해당 단어가 몇개의 문서에 나타났는지
  - IDF : DF의 역수 전체문서/DF
  - TF 도 높지만, DF 도 높으면

### BOW 구현 : countVectorizer를 이용한 피처 벡터화

- 사전데이터 가공 : 모든 문자를 소문자(default lowercase=true\_
- 토큰화 : n_gram_range를 반영
- 텍스트 정규화 : stop words 필터링만 수행
- 피처 벡터화 : max_df, min_df, max_features의 파라미터를 반영하여 토큰된 단어들을 feature extraction 후 vectorization 적용
  from sklearn.feature_extraction.text import CountVectorizer

'Count Vectorization으로 feature extraction 변환 수행.
cnt_vect = CountVectorizer()
cnt_vect.fit(text)

params: {'analyzer': 'word', 'binary': False, 'decode_error': 'strict', 'dtype': <class 'numpy.int64'>, 'encoding': 'utf-8', 'input': 'content', 'lowercase': True, 'max_df': 1.0, 'max_features': None, 'min_df': 1, 'ngram_range': (1, 1), 'preprocessor': None, 'stop_words': None, 'strip_accents': None, 'token_pattern': '(?u)\\b\\w\\w+\\b', 'tokenizer': None, 'vocabulary': None}

## 희소 행렬

- CSR Matrix : BOW의 벡토라이제이션 모델은 너무 많은 0의 ㄱ밧이 메모리 공간에 할당되어(모든 단어가 나오기 때문에) 너무 많은 시간, 공간이 소모된다
- 따라서 CSR 형식. 좌표형식 -> 0이 아닌 데이턱 값 배열, 행 과 열 위치 파악

dense3 = np.array([[0,0,1,0,0,5],
             [1,4,0,3,2,5],
             [0,6,0,3,0,0],
             [2,0,0,0,0,0],
             [0,0,0,7,0,8],
             [1,0,0,0,0,0]])

coo = sparse.coo_matrix(dense3)
csr = sparse.csr_matrix(dense3)